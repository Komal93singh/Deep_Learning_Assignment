{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Weight Initialization Techniques Assignment"
      ],
      "metadata": {
        "id": "GNCPOgKIITwh"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7OysShFfHz49"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q1. What is the vanishing gradient problem in deep neural networks? How does it affect training?"
      ],
      "metadata": {
        "id": "aGWI9tipIhzd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### **Vanishing Gradient Problem**:  \n",
        "The vanishing gradient problem occurs in deep neural networks when gradients shrink exponentially during backpropagation, particularly with activation functions like sigmoid or tanh. This leads to very small weight updates for earlier layers, hindering learning and slowing convergence.\n",
        "\n",
        "###**Effect on Training**:  \n",
        "- Slows or stalls training, especially in very deep networks.  \n",
        "- Impacts tasks requiring long-term dependencies (e.g., RNNs).  \n",
        "\n",
        "###**Solutions**:  \n",
        "1. **ReLU Activation**: Avoids vanishing gradients with a constant gradient for positive inputs.  \n",
        "2. **He Initialization**: Maintains gradient flow with proper weight initialization.  \n",
        "3. **Batch Normalization**: Stabilizes gradient ranges during training.  \n",
        "4. **Residual Networks (ResNets)**: Skip connections allow gradient flow across layers.  \n"
      ],
      "metadata": {
        "id": "2DxVvsErN-Da"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oKbSDHvDH0FJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q2. Explain how Xavier initialization addresses the vanishing gradient problem."
      ],
      "metadata": {
        "id": "juX_cCJmIxeS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Xavier Initialization and the Vanishing Gradient Problem**  \n",
        "\n",
        "Xavier (or Glorot) initialization addresses the vanishing gradient problem by ensuring the variance of activations and gradients remains stable across layers. It initializes weights using a distribution with zero mean and variance:  \n",
        "\n",
        "[ {Var}(W) = frac{2}/{n_{{in}} + n_{{out}}}]  \n",
        "\n",
        "where ( n_{{in}}) is the number of input units, and ( n_{{out}}) is the number of output units in a layer.  \n",
        "\n",
        "**Why It Works**:  \n",
        "1. **Preserves Variance**: Keeps the variance of outputs similar to inputs, avoiding the shrinking or exploding of gradients.  \n",
        "2. **Stable Gradients**: Balances weight scale to ensure stable gradient flow, particularly with sigmoid or tanh activations prone to saturation.  \n",
        "\n",
        "Xavier initialization adjusts the weight scale based on the number of inputs and outputs to each\n",
        "layer, helping to maintain a stable variance for both activations and gradients.\n",
        "This reduces the likelihood of vanishing gradients, especially in deep networks, leading to more\n",
        "efficient training."
      ],
      "metadata": {
        "id": "gvs-0No4PkzF"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MPgFEqPcIw3N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q3. What are some common activation functions that are prone to causing vanishing gradients?"
      ],
      "metadata": {
        "id": "BftcSeOYJBtx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Activation Functions Prone to Vanishing Gradients**  \n",
        "\n",
        "1. **Sigmoid**:  \n",
        "   - Squashes outputs to [0, 1].  \n",
        "   - Gradients become near zero when inputs are very large or very small (outputs near 0 or 1).  \n",
        "\n",
        "2. **Tanh**:  \n",
        "   - Squashes outputs to [-1, 1].  \n",
        "   - Gradients vanish when inputs are large, causing outputs close to -1 or 1.  \n",
        "\n",
        "These functions amplify the vanishing gradient effect in deep networks, slowing learning during backpropagation."
      ],
      "metadata": {
        "id": "yproi6xfRKJm"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7jHiXgDEH0Lq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q4. Define the exploding gradient problem in deep neural networks. How does it impact training?"
      ],
      "metadata": {
        "id": "v8t42rvtJNLZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exploding Gradient Problem**  \n",
        "The exploding gradient problem occurs when gradients grow exponentially during backpropagation, especially in deep networks. This leads to excessively large weight updates, destabilizing the training process.  \n",
        "\n",
        "### **Impact on Training**:  \n",
        "1. **Instability**: Weights may oscillate or diverge, causing the model to fail to converge.  \n",
        "2. **Numerical Issues**: Extremely large gradients can cause overflow, resulting in NaN values or crashes.  \n",
        "3. **Slow Convergence**: Overshooting optimal solutions delays or prevents convergence.  \n",
        "\n",
        "**Causes**:  \n",
        "- Improper weight initialization.  \n",
        "- Deep networks amplifying gradients.  \n",
        "\n",
        "**Solutions**:  \n",
        "1. **Gradient Clipping**: Caps gradients at a threshold to stabilize training.  \n",
        "2. **Weight Initialization**: Use Xavier or He initialization to control gradient scale.  \n",
        "3. **Batch Normalization**: Normalizes activations to reduce gradient instability.  \n"
      ],
      "metadata": {
        "id": "QOLA5yOgRu7f"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xDdvMrWuH0VS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q5. What is the role of proper weight initialization in training deep neural networks?"
      ],
      "metadata": {
        "id": "fq3QFFn9Jbfc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Role of Proper Weight Initialization in Training Deep Neural Networks**  \n",
        "\n",
        "1. **Prevents Vanishing/Exploding Gradients**: Proper initialization ensures gradients remain stable, avoiding vanishing (too small) or exploding (too large) values during backpropagation.  \n",
        "2. **Speeds Up Convergence**: Well-initialized weights allow the network to learn faster by starting close to a good solution, reducing training time.  \n",
        "3. **Enables Stable Learning**: By maintaining balanced activation and gradient variances, proper initialization ensures meaningful weight updates across all layers.  \n",
        "\n",
        "### **Techniques**:  \n",
        "- **Xavier Initialization**: For sigmoid/tanh activations, keeps variances controlled.  \n",
        "- **He Initialization**: For ReLU activations, maintains stable gradient flow.  \n",
        "\n",
        "Proper weight initialization ensures efficient and stable training, leading to better performance in deep networks."
      ],
      "metadata": {
        "id": "JkYUnIzeScaz"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8l5vNE80JazV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q6. Explain the concept of batch normalization and its impact on weight initialization techniques."
      ],
      "metadata": {
        "id": "Zd9hSur7Jo48"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Batch Normalization**  \n",
        "Batch normalization (BN) normalizes the activations of each layer to have a mean of zero and a standard deviation of one during training, using statistics computed from each mini-batch. This reduces internal covariate shift, stabilizing and speeding up training.  \n",
        "\n",
        "### **Impact on Weight Initialization**:  \n",
        "1. **Stabilizes Gradients**: By keeping activations in a controlled range, BN reduces the risk of vanishing or exploding gradients.  \n",
        "2. **Reduces Sensitivity**: BN makes the network less dependent on precise weight initialization, enabling more flexible initialization schemes (e.g., Xavier, He).  \n",
        "3. **Improves Convergence**: BN ensures smoother training and faster convergence, even with suboptimal weight initialization.  \n",
        "\n",
        "Overall, batch normalization complements weight initialization by improving training efficiency and robustness."
      ],
      "metadata": {
        "id": "Hbg-CFaSTQSn"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5-AsKsm2Ja8E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q7. Implement He initialization in python using TensorFlow or PyTorch."
      ],
      "metadata": {
        "id": "yftmDNiOJ1wY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can implement **He initialization** in both **TensorFlow** and **PyTorch**:\n",
        "\n",
        "---\n",
        "\n",
        "### **1. Using TensorFlow**\n",
        "TensorFlow provides a built-in initializer for He initialization called `tf.keras.initializers.HeNormal`. Here's how to use it:\n"
      ],
      "metadata": {
        "id": "SNNH98xFWBw3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Define a layer with He initialization\n",
        "layer = tf.keras.layers.Dense(\n",
        "    units=128,  # Number of neurons\n",
        "    activation='relu',\n",
        "    kernel_initializer=tf.keras.initializers.HeNormal()  # He Initialization\n",
        ")\n",
        "\n",
        "# Example usage in a model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.InputLayer(input_shape=(64,)),\n",
        "    tf.keras.layers.Dense(128, activation='relu', kernel_initializer=tf.keras.initializers.HeNormal()),\n",
        "    tf.keras.layers.Dense(64, activation='relu', kernel_initializer=tf.keras.initializers.HeNormal()),\n",
        "    tf.keras.layers.Dense(10, activation='softmax')  # Output layer\n",
        "])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        },
        "id": "sXugvGoEWAqA",
        "outputId": "70551ef5-9d81-4ade-8b1c-10f4eaad4d81"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/input_layer.py:26: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │           \u001b[38;5;34m8,320\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │           \u001b[38;5;34m8,256\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)                  │             \u001b[38;5;34m650\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │           <span style=\"color: #00af00; text-decoration-color: #00af00\">8,320</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">650</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m17,226\u001b[0m (67.29 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">17,226</span> (67.29 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m17,226\u001b[0m (67.29 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">17,226</span> (67.29 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Explanation**  \n",
        "- **TensorFlow**: `tf.keras.initializers.HeNormal()` initializes weights by drawing values from a normal distribution scaled by \\(\\sqrt{2 / \\text{fan_in}}\\), where `fan_in` is the number of input neurons."
      ],
      "metadata": {
        "id": "cCByjKexXGH0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### **2. Using PyTorch**\n",
        "In PyTorch, you can use `torch.nn.init.kaiming_normal_` for He initialization. Here's an example:"
      ],
      "metadata": {
        "id": "2qqivKnBWwvz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define a custom neural network with He initialization\n",
        "class CustomModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CustomModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(64, 128)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.fc3 = nn.Linear(64, 10)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        # Apply He initialization to the layers\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            nn.init.kaiming_normal_(module.weight, nonlinearity='relu')  # He initialization\n",
        "            if module.bias is not None:\n",
        "                nn.init.zeros_(module.bias)  # Initialize biases to zero\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Instantiate and test the model\n",
        "model = CustomModel()\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YDC83ek7WAwA",
        "outputId": "053673dc-d62c-4ab5-a95d-7bd4b95553ac"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CustomModel(\n",
            "  (fc1): Linear(in_features=64, out_features=128, bias=True)\n",
            "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
            "  (fc3): Linear(in_features=64, out_features=10, bias=True)\n",
            "  (relu): ReLU()\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Explanation**\n",
        "- **PyTorch**: `torch.nn.init.kaiming_normal_` implements the same scaling rule. The `nonlinearity='relu'` ensures it's optimized for ReLU activation functions.\n"
      ],
      "metadata": {
        "id": "0O2tAakuXMG8"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "L6B4qPo0XKr-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qgvkBCE9H0Yy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}