{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Forward and Backward Propagation Assignment**"
      ],
      "metadata": {
        "id": "g_6uGYhW1ASp"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2jzmD8le09iz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q1. Explain the concept of forward propagation in a neural network."
      ],
      "metadata": {
        "id": "gnj2u-QN11E7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### **Forward Propagation in Neural Networks**  \n",
        "Forward propagation is the process of passing input data through the neural network to compute the output or prediction based on current weights and biases.  \n",
        "\n",
        "### **Steps in Forward Propagation:**  \n",
        "1. **Input Layer**: Input data is fed into the network.  \n",
        "2. **Weighted Sum**: For each neuron in the hidden layer, a weighted sum of the inputs is computed.\n",
        "Each input feature is multiplied by a corresponding weight (which determines the importance of\n",
        "the feature), and then the bias term is added.\n",
        "\n",
        "  Mathematically, this is expressed as:\n",
        "   \n",
        "   z = w1x1+ w2x2+ w3x3 +.....+ b\n",
        "   \n",
        "   where w1, w2, w3 are the weights, x1, x2, x3...xn  are the input features, and, b is the bias.\n",
        "3. **Activation Function**: Apply an activation function (e.g., ReLU, sigmoid) to introduce non-linearity.  \n",
        "4. **Hidden Layers**: Repeat the weighted sum and activation for all hidden layers.  \n",
        "5. **Output Layer**: Generate the final output, such as probabilities (using softmax) or regression values.  \n",
        "\n",
        "**Summary**: Forward propagation calculates the output by propagating inputs through the network, layer by layer, using weighted sums and activation functions. This forms the basis for making predictions.  \n"
      ],
      "metadata": {
        "id": "Ewpqjmez3qUe"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Jiegm-3R09xl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q2. What is the purpose of the activation function in forward propagation?"
      ],
      "metadata": {
        "id": "Z72OKVQW2Cxd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### **Purpose of the Activation Function in Forward Propagation**  \n",
        "The activation function introduces non-linearity to the neural network, enabling it to model complex, non-linear relationships in data.  \n",
        "\n",
        "####**Key Points:**  \n",
        "1. **Avoids Linearity**: Without activation functions, the network behaves as a linear model, regardless of layers.  \n",
        "2. **Enables Complexity**: Non-linear activation functions (e.g., ReLU, sigmoid, tanh) allow the network to learn complex patterns and approximate non-linear functions.  \n",
        "3. **Improves Learning**: They enable feature extraction, hierarchical learning, and adaptation to varying levels of abstraction, enhancing generalization and prediction accuracy.  \n",
        "\n",
        "In summary, activation functions are essential for solving complex tasks like image recognition or natural language processing, as they transform simple linear computations into powerful non-linear models."
      ],
      "metadata": {
        "id": "hbUnq8CP43r1"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nweEfYhR2OXb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q3. Describe the steps involved in the backward propagation (backprogation) algorithm."
      ],
      "metadata": {
        "id": "4TebtBi32Puu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### **Steps in the backward (backpropagation) algorithm:**  \n",
        "Backpropagation is the process of updating the weights and biases of a neural network to minimize the error by propagating the loss backward.\n",
        "\n",
        "**Key Steps:**  \n",
        "1. **Forward Pass**: Compute the output of the network and calculate the loss using a loss function (e.g., MSE, Cross-Entropy).  \n",
        "2. **Backward Pass**:  \n",
        "   - **Output Layer**: Compute the gradient of the loss with respect to the output and backpropagate through the activation function.  \n",
        "   - **Hidden Layers**: Calculate gradients for each layer using the chain rule, propagating the error backward layer by layer.  \n",
        "3. **Weight Update**: Use the computed gradients to update weights and biases using an optimization algorithm (e.g., gradient descent).  \n",
        "In text form, the equation can be written as:  \n",
        "\n",
        "**\"The new weight (\\(w_{\\text{new}}\\)) is calculated by subtracting the product of the learning rate (\\(\\eta\\)) and the gradient of the loss (\\( \\frac{\\partial L}{\\partial w} \\)) from the old weight (\\(w_{\\text{old}}\\)).\"**  \n",
        "\n",
        "This represents the weight update rule in gradient descent.   \n",
        "   w_{new} = w_{old} - \\eta \\cdot \\frac{\\partial L}{\\partial w}\n",
        "    \n",
        "   where ( \\eta ) is the learning rate.  \n",
        "4. **Repeat**: Perform forward and backward passes iteratively for multiple epochs until the loss converges.\n",
        "\n",
        "**Summary**: Backpropagation calculates gradients to adjust weights and biases, minimizing error and improving the model's performance over iterations."
      ],
      "metadata": {
        "id": "r8SHVn-Y-4Zp"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tWT0Dq_s2Off"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q4. What is the purpose of the chain rule in backprogation?"
      ],
      "metadata": {
        "id": "SBqa-h3l2fjT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### **Purpose of the Chain Rule in Backpropagation:**  \n",
        "The chain rule is crucial in backpropagation because it enables the calculation of gradients for the loss function with respect to weights and biases in a neural network.  \n",
        "\n",
        "**Key Points:**  \n",
        "1. **Gradient Decomposition**: The chain rule breaks the gradient calculation into smaller, manageable parts by chaining the derivatives of activations and weights.  \n",
        "2. **Error Propagation**: It helps propagate the error backward through layers by computing how the loss at the output depends on the parameters of each layer.  \n",
        "3. **Efficient Updates**: This process ensures efficient gradient computation for deep networks, allowing proper updates to weights and biases to minimize loss.  \n",
        "\n",
        "**Summary**: The chain rule allows backpropagation to compute gradients layer by layer, making it feasible to train deep neural networks efficiently."
      ],
      "metadata": {
        "id": "0NFGM5ubAtZY"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b2wgr-D72o8J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q5. Implement the forward progation process for a simple neural network with one hidden layer using NumPy."
      ],
      "metadata": {
        "id": "_M1h5LXE2qFJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the activation function (Sigmoid in this case)\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "# Define the input, weights, and biases for a simple network\n",
        "# Assume input layer has 3 neurons, hidden layer has 4 neurons, and output layer has 1 neuron\n",
        "\n",
        "# Input (batch size = 2, features = 3)\n",
        "X = np.array([[0.1, 0.2, 0.3],\n",
        "              [0.4, 0.5, 0.6]])\n",
        "\n",
        "# Weights for the hidden layer (3 input neurons, 4 hidden neurons)\n",
        "W_hidden = np.random.randn(3, 4)\n",
        "\n",
        "# Biases for the hidden layer (4 neurons in the hidden layer)\n",
        "b_hidden = np.random.randn(4)\n",
        "\n",
        "# Weights for the output layer (4 hidden neurons, 1 output neuron)\n",
        "W_output = np.random.randn(4, 1)\n",
        "\n",
        "# Bias for the output layer (1 output neuron)\n",
        "b_output = np.random.randn(1)\n",
        "\n",
        "# Forward Propagation\n",
        "# Step 1: Calculate the weighted sum for the hidden layer\n",
        "Z_hidden = np.dot(X, W_hidden) + b_hidden\n",
        "\n",
        "# Step 2: Apply activation function to the hidden layer (using Sigmoid)\n",
        "A_hidden = sigmoid(Z_hidden)\n",
        "\n",
        "# Step 3: Calculate the weighted sum for the output layer\n",
        "Z_output = np.dot(A_hidden, W_output) + b_output\n",
        "\n",
        "# Step 4: Apply activation function to the output (Sigmoid for a binary classification task)\n",
        "A_output = sigmoid(Z_output)\n",
        "\n",
        "# Print the output\n",
        "print(\"Output of the network:\")\n",
        "print(A_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F6q1hylN2pAd",
        "outputId": "27a26b6d-d076-44f9-811c-cc60ac0473c5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output of the network:\n",
            "[[0.11254923]\n",
            " [0.10850343]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WSbQqA6Q2pCT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4_xJVcAm096K"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}