{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Various Neural Network Architect Assignment**"
      ],
      "metadata": {
        "id": "phrXQwUiou-k"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "u0CY8IruoMg6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q1. Describe the basic structure of a Feedforward Neural Network(FNN). What is the purpose of the activation function?"
      ],
      "metadata": {
        "id": "Zgmm86PWo9Cv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### **Structure of a Feedforward Neural Network (FNN):**\n",
        "**FNN** is one of the simplest types of artificial neural networks. It consists of three main layers:\n",
        "1. **Input Layer:** Accepts the raw input data.  \n",
        "2. **Hidden Layers:** Intermediate layers where computations occur. Each hidden layer consists of multiple neurons, and an FNN can have one or more hidden layers.  \n",
        "3. **Output Layer:** Produces the final output or prediction based on the processed data from hidden layers.  \n",
        "\n",
        "Data flows in one direction: **input → hidden layers → output**, with no loops or feedback.\n",
        "\n",
        "\n",
        "### **Purpose of Activation Function:**\n",
        "- Introduces **non-linearity**, enabling the network to learn and represent complex patterns.  \n",
        "- Without it, the FNN would be limited to modeling **linear relationships**, restricting its ability to solve non-linear problems.\n",
        "\n",
        "**Common Activation Functions:**\n",
        "1. **ReLU (Rectified Linear Unit):** Prevents vanishing gradients and accelerates training.  \n",
        "2. **Sigmoid:** Suitable for **binary classification** (outputs between 0 and 1).  \n",
        "3. **Tanh:** Zero-centered and scales outputs between -1 and 1, often used in hidden layers.  \n"
      ],
      "metadata": {
        "id": "xeLD39-Xs3IB"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LDOx4y9EoMpz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q2. Explain the role of convolutional layers in a CNN. Why are pooling layers commonly used, and what do they achieve?"
      ],
      "metadata": {
        "id": "9teScRtPpObe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Role of Convolutional Layers in CNNs:**\n",
        "Convolutional layers are the core of CNNs and are responsible for feature detection in input images by applying small filters (kernels) over the image.  \n",
        "- **Key Functions:**\n",
        "  1. Learn **spatial hierarchies** of patterns, from simple (e.g., edges) to complex (e.g., shapes).  \n",
        "  2. Reduce parameters compared to fully connected layers by using shared weights (filters).  \n",
        "  3. Preserve **spatial relationships** between pixels.\n",
        "\n",
        "### **Role of Pooling Layers:**\n",
        "Pooling layers typically follow convolutional layers and are used to downsample feature maps.  \n",
        "- **Key Benefits:**\n",
        "  1. **Dimensionality Reduction:** Decreases computational cost by reducing the spatial size of feature maps.  \n",
        "  2. **Translation Invariance:** Makes the network robust to small translations or distortions in the input.  \n",
        "\n",
        "- **Common Types of Pooling:**\n",
        "  1. **Max Pooling:** Retains the maximum value in each region, emphasizing strong features.  \n",
        "  2. **Average Pooling:** Computes the average value in each region, focusing on smoother feature representation.  \n",
        "\n",
        "**Max pooling** is more widely used due to its ability to highlight key features effectively.  \n",
        "\n"
      ],
      "metadata": {
        "id": "-mSd9gOBtxVG"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OYc_H2AspNz_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q3. What is the key characteristic that differentiates Recurrent Neural Networks (RNNS) form other neural networks? How does an RNN handle sequential data?"
      ],
      "metadata": {
        "id": "np7DoB8xpfhR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Key Characteristic of RNNs:**\n",
        "Recurrent Neural Networks (RNNs) differ from other neural networks due to their ability to handle **sequential data**. Unlike feedforward networks, RNNs have **cyclic connections**, enabling them to retain and use information from **previous time steps** through a hidden state.\n",
        "\n",
        "\n",
        "### **RNNs Handle Sequential Data:**\n",
        "1. **Memory:** RNNs maintain a hidden state that captures information from earlier time steps, making them suitable for tasks like time series forecasting, speech recognition, and language modeling.  \n",
        "2. **Backpropagation Through Time (BPTT):** RNNs update weights by applying backpropagation across the entire sequence, learning temporal dependencies.  \n",
        "3. **Challenges:** RNNs struggle with the **vanishing gradient problem**, limiting their ability to model long-term dependencies.\n",
        "\n"
      ],
      "metadata": {
        "id": "DgGjRogduxgX"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3PAtwwgjoMwG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q4. Discuss the components of a Long Short-Term Memory (LSTM) network. How does it address the vanishing gradient problem?"
      ],
      "metadata": {
        "id": "wFqDXEImp05v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Components of an LSTM Network:**\n",
        "Long short-term memory **(LSTM)** networks are a type of RNN designed to capture **long-term dependencies** and address the **vanishing gradient problem**. They achieve this with the following key components:  \n",
        "\n",
        "1. **Memory Cell:** Stores information over time, acting as the \"memory\" of the network.  \n",
        "2. **Forget Gate:** Controls which information from the cell state should be discarded.  \n",
        "3. **Input Gate:** Determines which new information should be added to the memory cell.  \n",
        "4. **Output Gate:** Decides what information from the memory cell is used as the output for the current time step.  \n",
        "\n",
        "\n",
        "### **LSTMs Address the Vanishing Gradient Problem:**\n",
        "In standard RNNs, gradients diminish as they are propagated through many time steps, limiting their ability to learn long-term dependencies.  \n",
        "- LSTMs use **gates** and **cell states** to regulate the flow of information.  \n",
        "- Gradients can flow **unimpeded through the memory cell**, ensuring that important information is preserved over long sequences.  \n",
        "\n",
        "This design enables LSTMs to retain and use relevant information efficiently, solving the vanishing gradient issue effectively.  \n",
        "\n"
      ],
      "metadata": {
        "id": "TcbE0kjdvR_5"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RTQVQdtNoM6l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q5. Describe the roles of the generator and discriminator in a Generative Adversarial Network (GAN). What is the training objective for each?"
      ],
      "metadata": {
        "id": "oFvZq_DZoun5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Roles in a GAN:**\n",
        "1. **Generator:**  \n",
        "   - **Role:** Creates synthetic data (e.g., images) from random noise to mimic real data.  \n",
        "   - **Objective:** Fool the discriminator by generating data that appears real.  \n",
        "\n",
        "2. **Discriminator:**  \n",
        "   - **Role:** Differentiates between real data (from the dataset) and fake data (from the generator).  \n",
        "   - **Objective:** Accurately classify real and fake data.\n",
        "\n",
        "### **Training Objective of a GAN:**\n",
        "- The GAN operates as a **minimax game**:\n",
        "  - The **generator** tries to minimize the discriminator's ability to distinguish real from fake data.  \n",
        "  - The **discriminator** tries to maximize its accuracy in identifying real versus fake data.  \n",
        "\n",
        "Over time, the generator improves at creating realistic data, while the discriminator becomes better at distinguishing them, ideally reaching an equilibrium where the fake data is indistinguishable from real data.  \n",
        "\n"
      ],
      "metadata": {
        "id": "dw1qYMHRwWKA"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uQt_HBV5qg-5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UkMsp7jYqgvC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}